# Backpropagation

Backpropagation 是一種能有效計算梯度的方法。

我們的網路：

$$x^n \to \text{NN}(\theta) \to y^n \leftrightarrow_{C^n} \hat y^n$$

## Gradient Descent Review

假設我們有以下參數：$\theta = \\{w_1, w_2, \dots, b_1, b_2, \dots\\}$

我們會逐次更新參數如下：$\theta^0 \to \theta^1 \to \theta^2 \to \cdots$

但因為有太多參數，所以我們必需「有效地」更新參數。

給定 loss function：

\begin{align}
L ( \theta ) & = \sum _ { n = 1 } ^ { N } C ^ { n } ( \theta ) \\\\
\to \frac { \partial L ( \theta ) } { \partial w } & = \sum _ { n = 1 } ^ { N } \frac { \partial C ^ { n } ( \theta ) } { \partial w }
\end{align}

## Backpropagation

![](https://imgur.com/CFHtwpb.png)

假設只考慮其中一個 neuron，則 $z = x_1w_1 + x_2w_2 + b$，我們所要求的是：

$$\frac { \partial C } { \partial w } = \frac { \partial z } { \partial w } \frac { \partial C } { \partial z }$$

透過 forward pass 和 backward pass 可分別算出：

- Forward pass: Compute $\partial z / \partial w$ for all parameters
- Backward pass: Compute $\partial C / \partial z$ for all activation function inputs $z$

### Forward pass

- $\partial z / \partial w_1 = x_1$
- $\partial z / \partial w_2 = x_2$

其實就是 input value 前面所接的值。

你要算出這個 neural network 裡面的每一個 weight 對它的 activation function 的 input $z$ 的偏微分，你就把你的 input 丟進去。然後，計算每一個 neuron 的 output 就結束了。

### Backward pass

要計算出 $\frac { \partial C } { \partial z }$ 是非常複雜的，但我們可以透過 chain rule 做些拆解：

![](https://imgur.com/C67xQpB.png)

\begin{align}
\frac { \partial C } { \partial z } & = \frac { \partial a } { \partial z } \frac { \partial C } { \partial a } \\\\
& = \sigma ^ { \prime } ( z ) \left[ \frac { \partial z ^ { \prime } } { \partial a } \frac { \partial C } { \partial z ^ { \prime } } + \frac { \partial z ^ { \prime \prime } } { \partial a } \frac { \partial C } { \partial z ^ { \prime \prime } } \right] \\\\
& = \sigma ^ { \prime } ( z ) \left[ w _ { 3 } \frac { \partial C } { \partial z ^ { \prime } } + w _ { 4 } \frac { \partial C } { \partial z ^ { \prime \prime } } \right]
\end{align}

想像有另一個 network，方向是反過來的，下面這個網路即和上式所代表的意義一樣：

![](https://imgur.com/R5CVdpT.png)

#### Case 1. Output Layer

![](https://imgur.com/jPZZPaa.png)

\begin{align}
\frac { \partial C } { \partial z ^ { \prime } } & = \frac { \partial y _ { 1 } } { \partial z ^ { \prime } } \frac { \partial C } { \partial y _ { 1 } } \\\\
\frac { \partial C } { \partial z ^ { \prime \prime } } & = \frac { \partial y _ { 2 } } { \partial z ^ { \prime \prime } } \frac { \partial C } { \partial y _ { 2 } }
\end{align}

#### Case 2. Not Output Layer

![](https://imgur.com/4LO37Jp.png)

Compute $\partial C / \partial z$ recursively until we reach the output layer.

## Summary

![](https://imgur.com/PIJqv1u.png)